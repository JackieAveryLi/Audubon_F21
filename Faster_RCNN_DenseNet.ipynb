{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_RCNN_DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "781188e7560c4e1d8f8fc95903d2105b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b13af2b12db460ca3bcf6a00c3948f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_76c6f5ec680244f599973a66b20b7646",
              "IPY_MODEL_78dbf0045a724be2a4697e579d61ca37",
              "IPY_MODEL_19d08e6cee464983866489ee177085a2"
            ]
          }
        },
        "0b13af2b12db460ca3bcf6a00c3948f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76c6f5ec680244f599973a66b20b7646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fd116f96619417986d42c9f847ab838",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Cropping files: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93c70cb2d2984c618aa8444e7c3bb0d4"
          }
        },
        "78dbf0045a724be2a4697e579d61ca37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1eab40aca20d4b6692eb6ede2a6e6807",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87394c7004ac4351972e926cdb259a1d"
          }
        },
        "19d08e6cee464983866489ee177085a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec8383627f2444ec9fba019ea5f405db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87/87 [05:21&lt;00:00,  4.35s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_057f6b2c171948d984789d82845de7b3"
          }
        },
        "8fd116f96619417986d42c9f847ab838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93c70cb2d2984c618aa8444e7c3bb0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1eab40aca20d4b6692eb6ede2a6e6807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87394c7004ac4351972e926cdb259a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec8383627f2444ec9fba019ea5f405db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "057f6b2c171948d984789d82845de7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import some needed libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import os, json, cv2, random\n",
        "import sys, shutil, glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import io  \n",
        "from datetime import datetime\n",
        "from distutils.dir_util import copy_tree"
      ],
      "metadata": {
        "id": "qF5tsb-xxH4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Detectron 2"
      ],
      "metadata": {
        "id": "9-5Ir-9yuVbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell only excecutes if you're running on Colab. \n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import drive \n",
        "  drive.mount('/gdrive/') # Mount Google Drive! \n",
        "\n",
        "  # Clone Audubon bird detection Github repo SP22 branch\n",
        "  !git clone -b SP22 https://github.com/RiceD2KLab/Audubon_F21.git\n",
        "\n",
        "  !pip install pyyaml==5.1\n",
        "\n",
        "  # This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
        "  !pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "  import torch\n",
        "  TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "  CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "  print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "  # Install detectron2 that matches the above pytorch version\n",
        "  # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "  !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "  # If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n",
        "\n",
        "  # exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "metadata": {
        "id": "CW47FI8iuUAc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35b4b049-0439-4820-c931-990e6bc5d143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive/\n",
            "Cloning into 'Audubon_F21'...\n",
            "remote: Enumerating objects: 1062, done.\u001b[K\n",
            "remote: Counting objects: 100% (1062/1062), done.\u001b[K\n",
            "remote: Compressing objects: 100% (638/638), done.\u001b[K\n",
            "remote: Total 1062 (delta 606), reused 759 (delta 392), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1062/1062), 60.22 MiB | 18.17 MiB/s, done.\n",
            "Resolving deltas: 100% (606/606), done.\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 7.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=824bed803ce5afb24881d4f1ae48798dfebe40b19154f9300dc0cd0c204d0ee9\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu102\n",
            "  Downloading https://download.pytorch.org/whl/cu102/torch-1.9.0%2Bcu102-cp37-cp37m-linux_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.9 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu102\n",
            "  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.10.0%2Bcu102-cp37-cp37m-linux_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu102) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu102) (1.21.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu102) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0+cu102 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.0+cu102 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0+cu102 torchvision-0.10.0+cu102\n",
            "torch:  1.9 ; cuda:  cu102\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
            "Collecting detectron2\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/detectron2-0.6%2Bcu102-cp37-cp37m-linux_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.4)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20220305.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.8.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.63.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Collecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting regex>=2020.1.8\n",
            "  Downloading regex-2022.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting toml>=0.10.1\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (5.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.11.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.2.0)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220305-py3-none-any.whl size=61214 sha256=b18949da24b1842e852ef9866ca593735109f439a73e993a9c23d8e8f04a9874\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b7/6e/43b1693d06fac3633af48db68557513b0a37ab38b0a8b798f9\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=6240e2f7a65ccb922cf8b872a6e53eace77d5c913ee6ea0b264c8c2f8193b260\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, toml, regex, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.6+cu102 fvcore-0.1.5.post20220305 hydra-core-1.1.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.1 pathspec-0.9.0 portalocker-2.4.0 regex-2022.3.2 toml-0.10.2 typed-ast-1.5.2 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Images"
      ],
      "metadata": {
        "id": "qX2vHftZ0yFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p './data/raw'\n",
        "\n",
        "# If downloading a public zip file\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!gdown -q https://drive.google.com/uc?id=1zhB6_MLtvD0JCoyKYqhUx497WIvSYVUk\n",
        "!unzip -q './1017_1.zip' -d './data/raw'\n",
        "!gdown -q https://drive.google.com/uc?id=1clRsR5zg60FYjQ-crGx8CN88yPsUgVse\n",
        "!unzip -q './1017_2.zip' -d './data/raw'\n",
        "!gdown -q https://drive.google.com/uc?id=1fC4xAZJFoEccrgBhvjLMGpzLVXEcfHm6\n",
        "!unzip -q './annotation_1017.zip' -d './data/raw'\n",
        "\n",
        "# # If zip files are contained in the mounted Google Drive\n",
        "# !unzip -q '/gdrive/MyDrive/1017_1.zip' -d './data/raw'\n",
        "# !unzip -q '/gdrive/MyDrive/1017_2.zip' -d './data/raw'\n",
        "# !unzip -q '/gdrive/MyDrive/annotation_1017.zip' -d './data/raw'"
      ],
      "metadata": {
        "id": "tEao3tio00d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23fac10-61c1-4af0-9c1d-d30be8ad42b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=be41de4be1a576fc776c70877d8e84de033fb91f8aa1406a2c27a80e02dc661f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-onby48sz/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display an image"
      ],
      "metadata": {
        "id": "bDachdFO1PFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = cv2.imread(\"./data/raw/DJI_20210520121129_0616.JPG\") \n",
        "cv2_imshow(im)"
      ],
      "metadata": {
        "id": "EWUCqovy1Tcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show data statistics"
      ],
      "metadata": {
        "id": "gyFNc_gi1zbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell plots the distribution of bird species contained in the entire dataset\n",
        "data_dir = './data/raw' # data directory folder \n",
        "\n",
        "# Load CSV files \n",
        "target_data = []\n",
        "for f in glob.glob(os.path.join(data_dir,'*.bbx')): \n",
        "  target_data.append(pd.read_csv(f, header=0, \n",
        "                              names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"]) )\n",
        "target_data = pd.concat(target_data, axis=0, ignore_index=True)\n",
        "\n",
        "# Create table and bar plot of bird species  \n",
        "print('\\n Bird Species Distribution')\n",
        "print(target_data[\"class_name\"].value_counts())\n",
        "print('\\n')\n",
        "\n",
        "ax = target_data[\"class_name\"].value_counts().plot.bar(x=\"Bird Species\", y=\"Frequency\",figsize=(10,6))  \n",
        "ax.set_title('Bird Species Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jR8-7QWf14Fm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "outputId": "0982b828-51fc-4458-9df0-62e0f403bcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Bird Species Distribution\n",
            "Mixed Tern Adult                   8641\n",
            "Laughing Gull Adult                3168\n",
            "Brown Pelican Adult                 496\n",
            "Mixed Tern Flying                   156\n",
            "Other Bird                           95\n",
            "Laughing Gull Flying                 86\n",
            "Brown Pelican - Wings Spread         29\n",
            "Trash/Debris                         23\n",
            "Great Egret/White Morph Adult        23\n",
            "Brown Pelican Juvenile               20\n",
            "Brown Pelican In Flight              17\n",
            "Brown Pelican Wings Spread           14\n",
            "Tri-Colored Heron Adult              11\n",
            "Brown Pelican Chick                   6\n",
            "Black Crowned Night Heron Adult       1\n",
            "Roseate Spoonbill Adult               1\n",
            "Name: class_name, dtype: int64\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot an image with bounding boxes"
      ],
      "metadata": {
        "id": "fr1uao6W17HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image \n",
        "from Audubon_F21.utils import plotting\n",
        "from Audubon_F21.utils.cropping import csv_to_dict \n",
        "\n",
        "annot_dict = csv_to_dict(csv_path = './data/raw/DJI_20210520121129_0616.bbx', annot_file_ext='bbx')\n",
        "annotation_lst = [list(x.values()) for x in annot_dict['bbox']]\n",
        "\n",
        "image_file = './data/raw/DJI_20210520121129_0616.JPG'\n",
        "assert os.path.exists(image_file)\n",
        "\n",
        "#Load the image\n",
        "image = Image.open(image_file)\n",
        "\n",
        "#Plot the Bounding Box\n",
        "print(\"Raw image with bounding boxes:\")\n",
        "plotting.plot_img_bbx(image, annotation_lst)"
      ],
      "metadata": {
        "id": "W-BnVgxC2FQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crop dataset"
      ],
      "metadata": {
        "id": "UTpCsU5e2Uil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Audubon_F21.utils.cropping import crop_dataset_trainer\n",
        "\n",
        "# data_dir is the path that contains both images and annotations (image: jpg; annotation: csv or bbx)\n",
        "data_dir = './data/raw' # data directory folder \n",
        "# output dir is the path where you want to output new files. Please use the folder you defined above.\n",
        "output_dir = './data/tiled'\n",
        "\n",
        "crop_dataset_trainer(data_dir, output_dir, annot_file_ext='bbx', crop_height=640, crop_width=640,\n",
        "                     sliding_size_x=600, sliding_size_y=600, compute_sliding_size=False)"
      ],
      "metadata": {
        "id": "6x2q-Vf72X1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "781188e7560c4e1d8f8fc95903d2105b",
            "0b13af2b12db460ca3bcf6a00c3948f8",
            "76c6f5ec680244f599973a66b20b7646",
            "78dbf0045a724be2a4697e579d61ca37",
            "19d08e6cee464983866489ee177085a2",
            "8fd116f96619417986d42c9f847ab838",
            "93c70cb2d2984c618aa8444e7c3bb0d4",
            "1eab40aca20d4b6692eb6ede2a6e6807",
            "87394c7004ac4351972e926cdb259a1d",
            "ec8383627f2444ec9fba019ea5f405db",
            "057f6b2c171948d984789d82845de7b3"
          ]
        },
        "outputId": "2f548316-4d33-4459-d4a4-9d2f1721f9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating output directory at: ./data/tiled\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "781188e7560c4e1d8f8fc95903d2105b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Cropping files:   0%|          | 0/87 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset into training, validation, and test sets"
      ],
      "metadata": {
        "id": "PoClnR1F3Dos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from Audubon_F21.utils.cropping import train_val_test_split\n",
        "\n",
        "# create a new output folder for train, val, test dataset\n",
        "# create three folders under the new output folder, with name 'train', 'val', 'test'\n",
        "!mkdir -p /content/data/split\n",
        "!mkdir -p /content/data/split/train\n",
        "!mkdir -p /content/data/split/val\n",
        "!mkdir -p /content/data/split/test\n",
        "\n",
        "# specify the folder directory where you have the tiled images (output_dir of the crop_dataset() function)\n",
        "file_dir = '/content/data/tiled'\n",
        "# output_dir is the new output folder you created in the cell above\n",
        "output_dir = '/content/data/split'\n",
        "# train is a percentage, the fraction of files for training\n",
        "train_frac = 0.8\n",
        "# val is a percentage, the fraction of files for validation\n",
        "val_frac = 0.1\n",
        "# the fraction for test is default to be 1-train-val\n",
        "train_val_test_split(file_dir, output_dir, train_frac=train_frac, val_frac=val_frac)"
      ],
      "metadata": {
        "id": "bQAIC_w43J0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show data distribution for each set"
      ],
      "metadata": {
        "id": "d_ZdIsw34T7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data directory folders \n",
        "data_dir = 'data/split'\n",
        "dirs = [d for d in os.listdir(data_dir)]\n",
        "\n",
        "# Load CSV files \n",
        "for d in dirs: \n",
        "  target_data = []\n",
        "  for f in glob.glob(os.path.join(data_dir,d,'*.csv')): \n",
        "    target_data.append(pd.read_csv(f, header=0, \n",
        "                              names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"]) )\n",
        "  target_data = pd.concat(target_data, axis=0, ignore_index=True)\n",
        "\n",
        "  # Visualize dataset \n",
        "  print(f'\\n {d} - Bird Species Distribution')\n",
        "  print(target_data[\"class_name\"].value_counts())\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "WCND3y6T4SS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca7c64a-e3d6-4732-8c27-3e62f48e53ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " test - Bird Species Distribution\n",
            "Mixed Tern Adult                 1076\n",
            "Laughing Gull Adult               338\n",
            "Brown Pelican Adult                63\n",
            "Laughing Gull Flying               12\n",
            "Mixed Tern Flying                  10\n",
            "Other Bird                          9\n",
            "Trash/Debris                        4\n",
            "Brown Pelican In Flight             2\n",
            "Brown Pelican - Wings Spread        2\n",
            "Great Egret/White Morph Adult       1\n",
            "Brown Pelican Juvenile              1\n",
            "Name: class_name, dtype: int64\n",
            "\n",
            "\n",
            "\n",
            " val - Bird Species Distribution\n",
            "Mixed Tern Adult                   1073\n",
            "Laughing Gull Adult                 340\n",
            "Brown Pelican Adult                  45\n",
            "Laughing Gull Flying                  9\n",
            "Mixed Tern Flying                     8\n",
            "Great Egret/White Morph Adult         5\n",
            "Other Bird                            5\n",
            "Brown Pelican Juvenile                4\n",
            "Brown Pelican - Wings Spread          3\n",
            "Trash/Debris                          2\n",
            "Brown Pelican Chick                   1\n",
            "Brown Pelican Wings Spread            1\n",
            "Black Crowned Night Heron Adult       1\n",
            "Tri-Colored Heron Adult               1\n",
            "Name: class_name, dtype: int64\n",
            "\n",
            "\n",
            "\n",
            " train - Bird Species Distribution\n",
            "Mixed Tern Adult                   7425\n",
            "Laughing Gull Adult                2869\n",
            "Brown Pelican Adult                 389\n",
            "Mixed Tern Flying                   122\n",
            "Other Bird                           96\n",
            "Laughing Gull Flying                 57\n",
            "Trash/Debris                         23\n",
            "Great Egret/White Morph Adult        22\n",
            "Brown Pelican - Wings Spread         19\n",
            "Brown Pelican Juvenile               15\n",
            "Brown Pelican Wings Spread           11\n",
            "Tri-Colored Heron Adult              11\n",
            "Brown Pelican In Flight               5\n",
            "Brown Pelican Chick                   3\n",
            "Black Crowned Night Heron Adult       1\n",
            "Roseate Spoonbill Adult               1\n",
            "Name: class_name, dtype: int64\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch"
      ],
      "metadata": {
        "id": "OpgM359-zeRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchvision, torch\n",
        "# from torchvision.models.detection import FasterRCNN\n",
        "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "# from torchvision.models.detection.rpn import AnchorGenerator"
      ],
      "metadata": {
        "id": "uXuMHf0GwITX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # load a model pre-trained pre-trained on COCO and extract only the features\n",
        "# backbone = torchvision.models.densenet121(pretrained=True).features\n",
        "# # FasterRCNN needs to know the number of\n",
        "# # output channels in a backbone. For densenet121, it's 1024\n",
        "# # so we need to add it here\n",
        "# backbone.out_channels = 1024\n",
        "\n",
        "# # let's make the RPN generate 3 x 3 anchors per spatial location, with 3 different sizes and 3 different aspect\n",
        "# # ratios. We have a Tuple[Tuple[int]] because each feature map could potentially have different sizes and aspect ratios\n",
        "# #by default the achor generator FasterRcnn assign will be for a FPN backone, so\n",
        "# #we need to specify a  different anchor generator\n",
        "# anchor_generator = AnchorGenerator(sizes=((128, 256, 512),),\n",
        "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# #here at each position in the grid there will be 3x3=9 anchors\n",
        "# #and if our backbone is not FPN then the forward method will assign the name '0' to feature map\n",
        "# #so we need to specify '0' as feature map name\n",
        "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "#                                                  output_size=7,\n",
        "#                                             sampling_ratio=2)\n",
        "# # Say we want to classify among 5 birds\n",
        "# num_classes = 5\n",
        "# #the output size is the output shape of the roi pooled features which will be used by the box head\n",
        "# model = FasterRCNN(backbone,num_classes=num_classes,rpn_anchor_generator=anchor_generator)"
      ],
      "metadata": {
        "id": "2qJho5vutuL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 600)]\n",
        "# predictions = model(x)"
      ],
      "metadata": {
        "id": "xG9NOQRryJFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Detectron2\n"
      ],
      "metadata": {
        "id": "Gn-PzFpkzm98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "WLaKBW00vB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup dataloaders \n",
        "\n",
        "The following cell registers the training, validation, and testing datasets with Detectron2's dataset catalogs. Note that we register both a version that utilizes both a singular \"bird-only\" label and the bird species labels. "
      ],
      "metadata": {
        "id": "0gSqqe0H64az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**See tutorial for creating a custome dataset.**"
      ],
      "metadata": {
        "id": "5aQMpbgl8NEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Audubon_F21.utils.dataloader import register_datasets\n",
        "\n",
        "data_dir = './data/split'\n",
        "img_ext='.JPEG'\n",
        "dirs = [os.path.join(data_dir,d) for d in os.listdir(data_dir)]\n",
        "\n",
        "# Bird species used by object detector. Species contained in dataset that are \n",
        "# not contained in this list will be categorized as an \"Unknown Bird\"\n",
        "BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
        "                \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
        "\n",
        "# Bounding box colors for bird species (used when plotting images)\n",
        "BIRD_SPECIES_COLORS = [(255,0,0), (255,153,51), (0, 255, 0), \n",
        "                       (0,0,255), (255, 51, 255)]\n",
        "\n",
        "register_datasets(dirs, img_ext, BIRD_SPECIES, bird_species_colors=BIRD_SPECIES_COLORS)"
      ],
      "metadata": {
        "id": "1-GjSBOd7CFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example function for getting dictionaries of custom data\n",
        "# dataset_dicts = get_balloon_dicts(\"balloon/train\")\n",
        "# for d in random.sample(dataset_dicts, 3):\n",
        "#     img = cv2.imread(d[\"file_name\"])\n",
        "#     visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n",
        "#     out = visualizer.draw_dataset_dict(d)\n",
        "#     cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "0OpuSDF4CQMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on a pretrained model"
      ],
      "metadata": {
        "id": "bPkaUL9K9FyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "\n",
        "model_name = \"faster_rcnn_R_50_FPN_1x\"   # ResNet-50 FPN backbone\n",
        "\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "outputs = predictor(im)"
      ],
      "metadata": {
        "id": "NEis7ZzM9C7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "print(outputs[\"instances\"].pred_classes) #### instances might not be the correct keyword\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ],
      "metadata": {
        "id": "m2QiBjU2-4uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # We can use `Visualizer` to draw the predictions on the image.\n",
        "# # Requires metadata registration\n",
        "# # For instance segmenation with Mask R-CNN, hence may not work exactly for Faster R-CNN\n",
        "# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "# cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "CA4X4Mb5_sks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tune a pretrained model"
      ],
      "metadata": {
        "id": "f637rt4R-rGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prexisting Backbone"
      ],
      "metadata": {
        "id": "YegbASI0bjcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Audubon_F21.utils.trainer import Trainer\n",
        "\n",
        "# setup training logger \n",
        "setup_logger()\n",
        "\n",
        "model_name = \"faster_rcnn_R_50_FPN_1x\"\n",
        "\n",
        "# Create detectron2 config \n",
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
        "# Get pretrained model from MS COCO\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
        "\n",
        "# add datasets used for training and validation \n",
        "cfg.DATASETS.TRAIN = (\"birds_species_train\",)\n",
        "cfg.DATASETS.TEST = (\"birds_species_val\",)   # val vs test?\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8   # what's a good number for 1 GPU?\n",
        "cfg.SOLVER.BASE_LR = 1e-3   # pick a good learning rate, 0.00025 is what tutorial uses\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "cfg.SOLVER.WARMUP_ITERS = 1\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES)\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.SOLVER.STEPS = [500,]   # [] to not decay the learning rate\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
        "\n",
        "cfg.OUTPUT_DIR = f\"./output/multibirds_{model_name}\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# train on bird species\n",
        "trainer = Trainer(cfg)   # DefaultTrainer(cfg) if not using a custom trainer\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "q1yDo8mqtL2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "metadata": {
        "id": "z_K_EmFbEQXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Backbone"
      ],
      "metadata": {
        "id": "VygxwpjjbtVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setup_logger()\n",
        "\n",
        "cfg = get_cfg()"
      ],
      "metadata": {
        "id": "0nV5f0A7bs8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model from yaml file"
      ],
      "metadata": {
        "id": "CE8Nd2mNfXjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load proper yaml config file, then\n",
        "model = build_model(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "Vk6sQrIFfZNF",
        "outputId": "6a2a466f-4d0a-4f74-9070-5fa7a8c42289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25f72378efc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load proper yaml config file, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model from config with additional argument overrides"
      ],
      "metadata": {
        "id": "lZqXvn-MfHMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask R-CNN\n",
        "model = GeneralizedRCNN(\n",
        "  cfg,\n",
        "  roi_heads=StandardROIHeads(cfg, batch_size_per_image=666),\n",
        "  pixel_std=[57.0, 57.0, 57.0])"
      ],
      "metadata": {
        "id": "-wiw0xbNfSLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model from full explicit arguments"
      ],
      "metadata": {
        "id": "1r86VgkYfSyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask R-CNN\n",
        "model = GeneralizedRCNN(\n",
        "    backbone=FPN(\n",
        "        ResNet(\n",
        "            BasicStem(3, 64, norm=\"FrozenBN\"),\n",
        "            ResNet.make_default_stages(50, stride_in_1x1=True, norm=\"FrozenBN\"),\n",
        "            out_features=[\"res2\", \"res3\", \"res4\", \"res5\"],\n",
        "        ).freeze(2),\n",
        "        [\"res2\", \"res3\", \"res4\", \"res5\"],\n",
        "        256,\n",
        "        top_block=LastLevelMaxPool(),\n",
        "    ),\n",
        "    proposal_generator=RPN(\n",
        "        in_features=[\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"],\n",
        "        head=StandardRPNHead(in_channels=256, num_anchors=3),\n",
        "        anchor_generator=DefaultAnchorGenerator(\n",
        "            sizes=[[32], [64], [128], [256], [512]],\n",
        "            aspect_ratios=[0.5, 1.0, 2.0],\n",
        "            strides=[4, 8, 16, 32, 64],\n",
        "            offset=0.0,\n",
        "        ),\n",
        "        anchor_matcher=Matcher([0.3, 0.7], [0, -1, 1], allow_low_quality_matches=True),\n",
        "        box2box_transform=Box2BoxTransform([1.0, 1.0, 1.0, 1.0]),\n",
        "        batch_size_per_image=256,\n",
        "        positive_fraction=0.5,\n",
        "        pre_nms_topk=(2000, 1000),\n",
        "        post_nms_topk=(1000, 1000),\n",
        "        nms_thresh=0.7,\n",
        "    ),\n",
        "    roi_heads=StandardROIHeads(\n",
        "        num_classes=80,\n",
        "        batch_size_per_image=512,\n",
        "        positive_fraction=0.25,\n",
        "        proposal_matcher=Matcher([0.5], [0, 1], allow_low_quality_matches=False),\n",
        "        box_in_features=[\"p2\", \"p3\", \"p4\", \"p5\"],\n",
        "        box_pooler=ROIPooler(7, (1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32), 0, \"ROIAlignV2\"),\n",
        "        box_head=FastRCNNConvFCHead(\n",
        "            ShapeSpec(channels=256, height=7, width=7), conv_dims=[], fc_dims=[1024, 1024]\n",
        "        ),\n",
        "        box_predictor=FastRCNNOutputLayers(\n",
        "            ShapeSpec(channels=1024),\n",
        "            test_score_thresh=0.05,\n",
        "            box2box_transform=Box2BoxTransform((10, 10, 5, 5)),\n",
        "            num_classes=80,\n",
        "        ),\n",
        "        mask_in_features=[\"p2\", \"p3\", \"p4\", \"p5\"],\n",
        "        mask_pooler=ROIPooler(14, (1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32), 0, \"ROIAlignV2\"),\n",
        "        mask_head=MaskRCNNConvUpsampleHead(\n",
        "            ShapeSpec(channels=256, width=14, height=14),\n",
        "            num_classes=80,\n",
        "            conv_dims=[256, 256, 256, 256, 256],\n",
        "        ),\n",
        "    ),\n",
        "    pixel_mean=[103.530, 116.280, 123.675],\n",
        "    pixel_std=[1.0, 1.0, 1.0],\n",
        "    input_format=\"BGR\",\n",
        ")"
      ],
      "metadata": {
        "id": "p-HjrwcDeolb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Detectron2"
      ],
      "metadata": {
        "id": "UuZKJvMR4NH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec"
      ],
      "metadata": {
        "id": "y69I7wvw4Zcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ToyBackbone(Backbone):\n",
        "#   def __init__(self, cfg, input_shape):\n",
        "#     super().__init__()\n",
        "#     # create your own backbone\n",
        "#     self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=16, padding=3)\n",
        "\n",
        "#   def forward(self, image):\n",
        "#     return {\"conv1\": self.conv1(image)}\n",
        "\n",
        "#   def output_shape(self):\n",
        "#     return {\"conv1\": ShapeSpec(channels=64, stride=16)}"
      ],
      "metadata": {
        "id": "p-2T8mLP4QbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # class DenseNet(nn.Module):\n",
        "# class DenseNet(Backbone):\n",
        "#     r\"\"\"Densenet-BC model class, based on\n",
        "#     `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "#     Args:\n",
        "#         growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "#         block_config (list of 4 ints) - how many layers in each pooling block\n",
        "#         num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "#         bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "#           (i.e. bn_size * k features in the bottleneck layer)\n",
        "#         drop_rate (float) - dropout rate after each dense layer\n",
        "#         num_classes (int) - number of classification classes\n",
        "#         memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "#           but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "#                  num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):\n",
        "\n",
        "#         super(DenseNet, self).__init__()\n",
        "\n",
        "#         # First convolution\n",
        "#         self.features = nn.Sequential(OrderedDict([\n",
        "#             ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
        "#                                 padding=3, bias=False)),\n",
        "#             ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "#             ('relu0', nn.ReLU(inplace=True)),\n",
        "#             ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "#         ]))\n",
        "\n",
        "#         # Each denseblock\n",
        "#         num_features = num_init_features\n",
        "#         for i, num_layers in enumerate(block_config):\n",
        "#             block = _DenseBlock(\n",
        "#                 num_layers=num_layers,\n",
        "#                 num_input_features=num_features,\n",
        "#                 bn_size=bn_size,\n",
        "#                 growth_rate=growth_rate,\n",
        "#                 drop_rate=drop_rate,\n",
        "#                 memory_efficient=memory_efficient\n",
        "#             )\n",
        "#             self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "#             num_features = num_features + num_layers * growth_rate\n",
        "#             if i != len(block_config) - 1:\n",
        "#                 trans = _Transition(num_input_features=num_features,\n",
        "#                                     num_output_features=num_features // 2)\n",
        "#                 self.features.add_module('transition%d' % (i + 1), trans)\n",
        "#                 num_features = num_features // 2\n",
        "\n",
        "#         # Final batch norm\n",
        "#         self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "#         # Linear layer\n",
        "#         self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "#         # Official init from torch repo.\n",
        "#         for m in self.modules():\n",
        "#             if isinstance(m, nn.Conv2d):\n",
        "#                 nn.init.kaiming_normal_(m.weight)\n",
        "#             elif isinstance(m, nn.BatchNorm2d):\n",
        "#                 nn.init.constant_(m.weight, 1)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, nn.Linear):\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         features = self.features(x)\n",
        "#         out = F.relu(features, inplace=True)\n",
        "#         out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "#         out = torch.flatten(out, 1)\n",
        "#         out = self.classifier(out)\n",
        "#         return out\n",
        "\n",
        "#     def output_shape(self):\n",
        "#         return {\"conv1\": ShapeSpec(channels=64, stride=16)}"
      ],
      "metadata": {
        "id": "zYDqklPQ4pOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet PyTorch Source Code"
      ],
      "metadata": {
        "id": "dNKfd1IO5GYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec, build_model"
      ],
      "metadata": {
        "id": "bFDmrOdYEuqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from collections import OrderedDict\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch import Tensor\n",
        "from torch.jit.annotations import List\n",
        "\n",
        "\n",
        "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
        "\n",
        "model_urls = {\n",
        "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
        "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
        "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
        "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
        "}"
      ],
      "metadata": {
        "id": "ajEh89wr5Mef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                                           growth_rate, kernel_size=1, stride=1,\n",
        "                                           bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False)),\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
        "        return bottleneck_output\n",
        "\n",
        "    # todo: rewrite when torchscript supports any\n",
        "    def any_requires_grad(self, input):\n",
        "        # type: (List[Tensor]) -> bool\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @torch.jit.unused  # noqa: T484\n",
        "    def call_checkpoint_bottleneck(self, input):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input)\n",
        "\n",
        "    @torch.jit._overload_method  # noqa: F811\n",
        "    def forward(self, input):\n",
        "        # type: (List[Tensor]) -> (Tensor)\n",
        "        pass\n",
        "\n",
        "    @torch.jit._overload_method  # noqa: F811\n",
        "    def forward(self, input):\n",
        "        # type: (Tensor) -> (Tensor)\n",
        "        pass\n",
        "\n",
        "    # torchscript does not yet support *args, so we overload method\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\n",
        "    def forward(self, input):  # noqa: F811\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "@BACKBONE_REGISTRY.register()\n",
        "# class DenseNet(nn.Module):\n",
        "class DenseNetBackbone(Backbone):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "    \"\"\"\n",
        "    # class ToyBackbone(Backbone):\n",
        "    #   def __init__(self, cfg, input_shape):\n",
        "    #     super().__init__()\n",
        "    #     # create your own backbone\n",
        "    #     self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=16, padding=3)\n",
        "\n",
        "    #   def forward(self, image):\n",
        "    #     return {\"conv1\": self.conv1(image)}\n",
        "\n",
        "    #   def output_shape(self):\n",
        "    #     return {\"conv1\": ShapeSpec(channels=64, stride=16)}\n",
        "\n",
        "\n",
        "\n",
        "    # def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "    #              num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):\n",
        "\n",
        "\n",
        "    # def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
        "    #             **kwargs):\n",
        "    #   model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "    #   if pretrained:\n",
        "    #       _load_state_dict(model, model_urls[arch], progress)\n",
        "    #   return model\n",
        "\n",
        "    _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "    def __init__(self, cgf, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False,\n",
        "                 pretrained = True, arch = 'densenet161', progress=True):\n",
        "\n",
        "        # super(DenseNet, self).__init__()\n",
        "        super().__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
        "                                padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if pretrained:\n",
        "            _load_state_dict(model, model_urls[arch], progress)\n",
        "            return model\n",
        "\n",
        "    # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=16, padding=3)\n",
        "\n",
        "    # def forward(self, image):\n",
        "    #     return {\"conv1\": self.conv1(image)}\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        # out = F.relu(features, inplace=True)\n",
        "        # out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        # out = torch.flatten(out, 1)\n",
        "        # out = self.classifier(out)\n",
        "        # return out\n",
        "        return features\n",
        "\n",
        "    def output_shape(self):\n",
        "        return {\"conv1\": ShapeSpec(channels=64, stride=16)}\n",
        "\n",
        "\n",
        "def _load_state_dict(model, model_url, progress):\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\n",
        "    # to find such keys.\n",
        "    pattern = re.compile(\n",
        "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "    for key in list(state_dict.keys()):\n",
        "        res = pattern.match(key)\n",
        "        if res:\n",
        "            new_key = res.group(1) + res.group(2)\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
        "              **kwargs):\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "    if pretrained:\n",
        "        _load_state_dict(model, model_urls[arch], progress)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet121(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-121 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "    \"\"\"\n",
        "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def densenet161(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-161 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "    \"\"\"\n",
        "    return _densenet('densenet161', 48, (6, 12, 36, 24), 96, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def densenet169(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-169 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "    \"\"\"\n",
        "    return _densenet('densenet169', 32, (6, 12, 32, 32), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def densenet201(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-201 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "    \"\"\"\n",
        "    return _densenet('densenet201', 32, (6, 12, 48, 32), 64, pretrained, progress,\n",
        "                     **kwargs)"
      ],
      "metadata": {
        "id": "x6A5OGWS5N5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_logger()\n",
        "cfg = get_cfg()\n",
        "\n",
        "model_name = \"faster_rcnn_R_50_FPN_1x\"\n",
        "\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
        "\n",
        "cfg.MODEL.BACKBONE.NAME = 'DenseNetBackbone'   # or set it in the config file\n",
        "\n",
        "model = build_model(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "pH3HTEPk6KkS",
        "outputId": "efa93039-8ead-41e9-d55d-5578f42501d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f572c0ab7870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DenseNetBackbone2'\u001b[0m   \u001b[0;31m# or set it in the config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/build.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0mmeta_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETA_ARCHITECTURE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMETA_ARCH_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0m_log_api_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modeling.meta_arch.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmeta_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/config/config.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_called_with_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mexplicit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_args_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_config_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexplicit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/config/config.py\u001b[0m in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_arg_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mextra_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_config_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;31m# forward the other arguments to __init__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         return {\n\u001b[1;32m     74\u001b[0m             \u001b[0;34m\"backbone\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/modeling/backbone/build.py\u001b[0m in \u001b[0;36mbuild_backbone\u001b[0;34m(cfg, input_shape)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbackbone_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBACKBONE_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackbone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-a0a221693e6a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, growth_rate, block_config, num_init_features, bn_size, drop_rate, num_classes, memory_efficient)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mgrowth_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mdrop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mmemory_efficient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'denseblock%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-a0a221693e6a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             layer = _DenseLayer(\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mnum_input_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mgrowth_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mbn_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'tuple'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # setup training logger \n",
        "# setup_logger()\n",
        "\n",
        "# model_name = \"faster_rcnn_R_50_FPN_1x\"\n",
        "\n",
        "# # Create detectron2 config \n",
        "# cfg = get_cfg()\n",
        "# # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "# cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
        "# # Get pretrained model from MS COCO\n",
        "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
        "\n",
        "# add datasets used for training and validation \n",
        "cfg.DATASETS.TRAIN = (\"birds_species_train\",)\n",
        "cfg.DATASETS.TEST = (\"birds_species_val\",)   # val vs test?\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8   # what's a good number for 1 GPU?\n",
        "cfg.SOLVER.BASE_LR = 1e-3   # pick a good learning rate, 0.00025 is what tutorial uses\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "cfg.SOLVER.WARMUP_ITERS = 1\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES)\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "cfg.SOLVER.STEPS = [500,]   # [] to not decay the learning rate\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n"
      ],
      "metadata": {
        "id": "_cgOXZfPGosb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.OUTPUT_DIR = f\"./output/multibirds_{model_name}\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "from Audubon_F21.utils.trainer import Trainer\n",
        "# train on bird species\n",
        "trainer = Trainer(cfg)   # DefaultTrainer(cfg) if not using a custom trainer\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qo6MkUvKIgWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference & evaluation using the trained model\n",
        "Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:"
      ],
      "metadata": {
        "id": "dgcrudY-ErG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "diZK_TgqErnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also evaluate its performance using AP metric implemented in COCO API."
      ],
      "metadata": {
        "id": "UNwNZZQzNQhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From tutorial\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"bird_species_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"bird_species_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ],
      "metadata": {
        "id": "iHZiOv97NYe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Team Audubon Fall 21 created the following functions for performance."
      ],
      "metadata": {
        "id": "87wJHM11RyAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "from Audubon_F21.utils.evaluation import plot_precision_recall\n",
        "from Audubon_F21.utils.evaluation import get_precisions_recalls\n",
        "\n",
        "print('validation inference:')\n",
        "val_precisions, val_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_species_val\")\n",
        "plot_precision_recall(val_precisions, val_max_recalls, BIRD_SPECIES + [\"Unknown Bird\"],\n",
        "                      BIRD_SPECIES_COLORS + [(0, 0, 0)])\n",
        "\n",
        "print('test inference:')\n",
        "test_precisions, test_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_species_test\")\n",
        "plot_precision_recall(test_precisions, test_max_recalls, BIRD_SPECIES + [\"Unknown Bird\"],\n",
        "                      BIRD_SPECIES_COLORS + [(0, 0, 0)])\n",
        "\n",
        "# Plot examples of detections on validation and testing tiled images \n",
        "for d in [\"val\", \"test\"]:\n",
        "    dataset_dicts = DatasetCatalog.get(f\"birds_species_{d}\")\n",
        "    metadata_set = MetadataCatalog.get(f\"birds_species_{d}\")\n",
        "    print(f'\\n {d} examples:')\n",
        "    for (i,k) in enumerate(random.sample(dataset_dicts, 2)):   # 2 samples\n",
        "        im = cv2.imread(k[\"file_name\"])\n",
        "        outputs = predictor(im)    # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "        outputs = outputs[\"instances\"].to(\"cpu\")\n",
        "        outputs = outputs[outputs.scores > 0.5]   # tutorial doesn't remove these; keep line?\n",
        "        v = Visualizer(im[:, :, ::-1],\n",
        "                        metadata=metadata_set,\n",
        "                        scale=0.5,\n",
        "                        instance_mode=ColorMode.SEGMENTATION)\n",
        "        out = v.draw_instance_predictions(outputs)\n",
        "        cv2.imshow(f'{d} prediction {i}',out.get_image()[:, :, ::-1])\n",
        "        v2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "eR4O7L2iHaWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on Test Set"
      ],
      "metadata": {
        "id": "LcoVq9W2TLfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tile the test set\n",
        "\n",
        "The tiling step in the detection pipeline is done using a sliding window. The sub-images are deliberately generated to have a significant proportion of overlapping with adjacent sub-images. We want to have the overlapping to ensure that there is at least one complete version of each bird in one of the sub-images. We then try to eliminate overlapping predicted bounding boxes for the same bird by using non-maximum suppression."
      ],
      "metadata": {
        "id": "Rwhnti2KTWN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Audubon_F21.utils.cropping import crop_dataset_img_only\n",
        "\n",
        "# create folder to contain tiled images\n",
        "!rm -rf './data/crop'\n",
        "!mkdir -p './data/crop'\n",
        "\n",
        "# perform tiling on images \n",
        "# data_dir = './data/raw'   # data directory folder \n",
        "data_dir = './data/split/test'\n",
        "output_dir = './data/crop'\n",
        "img_ext = '.JPG'\n",
        "CROP_WIDTH = 640 \n",
        "CROP_HEIGHT = 640\n",
        "SLIDING_SIZE = 400 \n",
        "crop_dataset_img_only(data_dir, img_ext, output_dir, crop_height=CROP_HEIGHT, crop_width=CROP_WIDTH, sliding_size=SLIDING_SIZE)"
      ],
      "metadata": {
        "id": "jtvLxOM5TOC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run pipeline"
      ],
      "metadata": {
        "id": "t33hz__vXt_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from Audubon_F21.utils.evaluation import evaluate_full_pipeline\n",
        "\n",
        "# create list of tiled images to be run predictor on \n",
        "eval_file_lst = []\n",
        "eval_file_lst = eval_file_lst + glob.glob('./data/crop/*.JPEG')\n",
        "\n",
        "# Create detectron2 config and predictor \n",
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model; tutorial uses 0.7\n",
        "\n",
        "# download model weights\n",
        "!gdown -q https://drive.google.com/uc?id=1-f_INg5D0yG7AJUkuSJUcIl6BSaf-smR    # currently previous team's weights\n",
        "# load model weights \n",
        "cfg.MODEL.WEIGHTS = \"./model_final.pth\"   # I'm assuming the downloaded weights were saved as 'model_final.pth'\n",
        "\n",
        "BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
        "                \"Great Blue Heron\",\"Great Egret/White Morph\"]   # not really used\n",
        "SPECIES_MAP = {0: 'Brown Pelican', 1: 'Laughing Gull', 2: 'Mixed Tern', 3: 'Great Blue Heron',\n",
        "               4: 'Great Egret/White Morph', 5: 'Other/Unknown'}\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES) \n",
        "\n",
        "# Create default predictor to run inference \n",
        "predictor = DefaultPredictor(cfg)\n",
        "RAW_IMG_WIDTH = 8192\n",
        "RAW_IMG_HEIGHT = 5460\n",
        "\n",
        "# Run evaluation \n",
        "output_df = evaluate_full_pipeline(eval_file_lst, predictor, SPECIES_MAP, RAW_IMG_WIDTH, RAW_IMG_HEIGHT,\n",
        "                           CROP_WIDTH, CROP_HEIGHT, SLIDING_SIZE)"
      ],
      "metadata": {
        "id": "5n1a-HWcXv6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download annotations as CSV file"
      ],
      "metadata": {
        "id": "47rfjZ1BX3qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "output_df.to_csv('output.csv')\n",
        "files.download('output.csv')"
      ],
      "metadata": {
        "id": "-z74qkrrX2sO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}