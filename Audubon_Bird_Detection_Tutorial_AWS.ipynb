{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RiceD2KLab/Audubon_F21/blob/main/Audubon_Bird_Detection_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwP9RpOZ21zr"
   },
   "source": [
    "# Houston Audubon Bird Detection Tutorial \n",
    "Authors: Krish Kabra, Alexander Xiong, Minxuan Luo, William Lu \n",
    "\n",
    "This Colab notebook contains tutorial code to perform bird detection using drone imagery based on the work done by Houston Audubon and students from the D2K capstone project course at Rice University. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj5EpyIO21Fp"
   },
   "source": [
    "## Installation and setup for Colab\n",
    "\n",
    "Run the next cells to setup Colab with the necessary requirements. We clone the Github repo with the developed code, and install dependencies, namely Detectron2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PFNsJ_Tek3A"
   },
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "import os, sys, shutil, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import cv2\n",
    "from skimage import io  \n",
    "from datetime import datetime\n",
    "from distutils.dir_util import copy_tree\n",
    "import boto3\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZK9R5DgH2jBr"
   },
   "outputs": [],
   "source": [
    "# This cell only excecutes if you're running on Colab. \n",
    "# if 'google.colab' in sys.modules:\n",
    "#   from google.colab import drive \n",
    "#   drive.mount('/gdrive/') # Mount Google Drive! \n",
    "\n",
    "  # Clone Audubon bird detection Github repo \n",
    "#   !git clone https://github.com/RiceD2KLab/Audubon_F21.git \n",
    "\n",
    "  # Install dependencies \n",
    "  !pip install -qq pyyaml==5.1\n",
    "  # This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
    "  !pip install -qq torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "  # Install detectron2 that matches the above pytorch version\n",
    "  # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "  !pip install -qq detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
    "  # exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime\n",
    "\n",
    "  !pip install -qq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHHm2ikxC99D"
   },
   "source": [
    "## Data exploration & wrangling\n",
    "\n",
    "The following cells contain the data exploration and wrangling modules of the data science pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m81HuKnD-J8"
   },
   "source": [
    "### Load dataset from Google Drive \n",
    "\n",
    "The following cell unzips a folder stored on Google Drive ontop the Colab machine. You can modify this cell to load your drone images onto the Colab instance! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra1KJejrGdbn"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p './data/raw'\n",
    "# !gdown -q https://drive.google.com/uc?id=1zhB6_MLtvD0JCoyKYqhUx497WIvSYVUk\n",
    "!aws s3 unzip -q 's3://sagemaker-studio-zd1j05seaof/1017_1' -d 's3://sagemaker-studio-zd1j05seaof/data'\n",
    "# !gdown -q https://drive.google.com/uc?id=1clRsR5zg60FYjQ-crGx8CN88yPsUgVse\n",
    "# !unzip -q './1017_2.zip' -d './data/raw'\n",
    "# !gdown -q https://drive.google.com/uc?id=1fC4xAZJFoEccrgBhvjLMGpzLVXEcfHm6\n",
    "# !unzip -q './annotation_1017.zip' -d './data/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker import get_execution_role\n",
    "# role = get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.resource('s3')\n",
    "# for bucket in s3.buckets.all():\n",
    "#     print(bucket.name)\n",
    "    \n",
    "# my_bucket = 'sagemaker-studio-zd1j05seaof'\n",
    "# my_file = '1017_1/DJI_20210520122305_0032.JPG'\n",
    "# s3client = boto3.client('s3')\n",
    "# response = s3client.get_object(Bucket=my_bucket, Key=my_file)\n",
    "# body = response['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# im = Image.open(body)\n",
    "# image = np.array(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (20,10))\n",
    "# plt.imshow(image[500:1300, 500:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to load up the file path for the annotation\n",
    "my_bucket = 'sagemaker-studio-zd1j05seaof'\n",
    "\n",
    "conn = boto3.client('s3')\n",
    "annot_path = 'annotation_1017/'\n",
    "annot = conn.list_objects(Bucket=my_bucket, Prefix=annot_path)['Contents']\n",
    "# for f in annot:\n",
    "# #     print(f['Key']) # print what files are in this specific folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot[1]['Key'].replace('.bbx', '.ck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA32BBX_JX-r"
   },
   "source": [
    "### Data exploration on AWS\n",
    "\n",
    "The following cells generate some metrics and plots to help understand the loaded dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = []\n",
    "for f in annot:\n",
    "    response = s3client.get_object(Bucket=my_bucket, Key=f['Key'])\n",
    "    body = response['Body']\n",
    "    target_data.append(pd.read_csv(body, header=0, \n",
    "                       names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"] ))\n",
    "target_data = pd.concat(target_data, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_data\n",
    "print('\\n Bird Species Distribution')\n",
    "print(target_data[\"class_name\"].value_counts())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = target_data[\"class_name\"].value_counts().plot.bar(figsize=(10,6))  \n",
    "ax.set_title('Bird Species Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jj_VoOC3Jg4A",
    "outputId": "03d25bbf-8fc5-4505-d68e-ddde07936ffa"
   },
   "outputs": [],
   "source": [
    "# Show an example image with corresponding bounding boxes \n",
    "from PIL import Image \n",
    "from Audubon_F21.utils import plotting\n",
    "from Audubon_F21.utils.cropping import csv_to_dict_AWS \n",
    "\n",
    "annot_dict = csv_to_dict(csv_path = './data/raw/DJI_20210520122307_0033.bbx', annot_file_ext='bbx')\n",
    "annotation_lst = [list(x.values()) for x in annot_dict['bbox']]\n",
    "\n",
    "image_file = './data/raw/DJI_20210520122307_0033.JPG'\n",
    "assert os.path.exists(image_file)\n",
    "\n",
    "#Load the image\n",
    "image = Image.open(image_file)\n",
    "\n",
    "#Plot the Bounding Box\n",
    "print(\"Raw image with bounding boxes:\")\n",
    "plotting.plot_img_bbx(image, annotation_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1srbJOyFEA9u"
   },
   "source": [
    "### Tiling \n",
    "\n",
    "In order to prepare the dataset to be used for training in our deep learning models, we must tile the large 8192 × 5460 raw drone images into smaller sizes. The size of generated images can be specified by setting parameters and is default to be 640 × 640.\n",
    "\n",
    "The following cells tiles the original dataset images and corresponding annotations in annotation files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfSuT1ioQFOX"
   },
   "outputs": [],
   "source": [
    "from Audubon_F21.utils.cropping import crop_dataset\n",
    "\n",
    "# data_dir is the path that contains both images and annotations (image: jpg; annotation: csv or bbx)\n",
    "data_dir = './data/raw' # data directory folder \n",
    "# output dir is the path where you want to output new files. Please use the folder you defined above.\n",
    "output_dir = './data/tiled'\n",
    "\n",
    "crop_dataset(data_dir, output_dir, annot_file_ext = 'bbx', crop_height = 640, crop_width = 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H0_4KdPEEJZ"
   },
   "source": [
    "### Split dataset into training, validation, and test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfb8Q6BtQQ5F"
   },
   "outputs": [],
   "source": [
    " from Audubon_F21.utils.cropping import train_val_test_split\n",
    "\n",
    "# create a new output folder for train, val, test dataset\n",
    "# create three folders under the new output folder, with name 'train', 'val', 'test'\n",
    "!mkdir -p /content/data/split\n",
    "!mkdir -p /content/data/split/train\n",
    "!mkdir -p /content/data/split/val\n",
    "!mkdir -p /content/data/split/test\n",
    "\n",
    "# specify the folder directory where you have the tiled images (output_dir of the crop_dataset() function)\n",
    "file_dir = '/content/data/tiled'\n",
    "# output_dir is the new output folder you created in the cell above\n",
    "output_dir = '/content/data/split'\n",
    "# train is a percentage, the fraction of files for training\n",
    "train_frac = 0.8\n",
    "# val is a percentage, the fraction of files for validation\n",
    "val_frac = 0.1\n",
    "# the fraction for test is default to be 1-train-val\n",
    "train_val_test_split(file_dir, output_dir, train_frac=train_frac, val_frac=val_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gEL7rjsEsSk"
   },
   "outputs": [],
   "source": [
    "# Distribution of bird species for train, val, and test sets\n",
    "# data directory folders \n",
    "data_dir = 'data/split'\n",
    "dirs = [d for d in os.listdir(data_dir)]\n",
    "\n",
    "# Load CSV files \n",
    "for d in dirs: \n",
    "  target_data = []\n",
    "  for f in glob.glob(os.path.join(data_dir,d,'*.csv')): \n",
    "    target_data.append(pd.read_csv(f, header=0, \n",
    "                              names = [\"class_id\", \"class_name\", \"x\", \"y\", \"width\", \"height\"]) )\n",
    "  target_data = pd.concat(target_data, axis=0, ignore_index=True)\n",
    "\n",
    "  # Visualize dataset \n",
    "  print(f'\\n {d} - Bird Species Distribution')\n",
    "  print(target_data[\"class_name\"].value_counts())\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_moYO6MEsyq"
   },
   "source": [
    "## Modeling \n",
    "\n",
    "The primary models used to detect birds within the drone images are convolutional neural network (CNN) based object detectors. To implement these models, we utilize [Detectron2](https://github.com/facebookresearch/detectron2), Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU-y99FBF7nm"
   },
   "source": [
    "### Setup dataloaders \n",
    "\n",
    "The following cell registers the training, validation, and testing datasets with Detectron2's dataset catalogs. Note that we register both a version that utilizes both a singular \"bird-only\" label and the bird species labels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5ruFu59cxbp"
   },
   "outputs": [],
   "source": [
    "from Audubon_F21.utils.dataloader import register_datasets\n",
    "\n",
    "data_dir = './data/split'\n",
    "img_ext='.JPEG'\n",
    "dirs = [os.path.join(data_dir,d) for d in os.listdir(data_dir)]\n",
    "\n",
    "# Bird species used by object detector. Species contained in dataset that are \n",
    "# not contained in this list will be categorized as an \"Unknown Bird\"\n",
    "BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
    "                \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
    "\n",
    "# Bounding box colors for bird species (used when plotting images)\n",
    "BIRD_SPECIES_COLORS = [(255,0,0), (255,153,51), (0, 255, 0), \n",
    "                       (0,0,255), (255, 51, 255)]\n",
    "\n",
    "register_datasets(dirs,img_ext,BIRD_SPECIES,bird_species_colors=BIRD_SPECIES_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVGJPdoKF-mj"
   },
   "source": [
    "### Training \n",
    "\n",
    "The following cells train a RetinaNet and Faster R-CNN model with a ResNet-50 FPN backbone. The model weights are initialized from a model pretrained on the MS COCO dataset. The training loop is based on Detectron2's Default Trainer.  Hyperparameters can be tweaked! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq7HuVsoKQET"
   },
   "source": [
    "#### Bird-only model\n",
    "\n",
    "The bird-only model simplies localizes all birds and does not distiguish bird species. We utilize RetinaNet for faster performance rather than accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7pAw0SSKkh5"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from Audubon_F21.utils.trainer import Trainer\n",
    "\n",
    "# setup training logger \n",
    "setup_logger()\n",
    "\n",
    "model_name = \"retinanet_R_50_FPN_1x\"\n",
    "\n",
    "# Create detectron2 config \n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
    "# Get pretrained model from MS COCO\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
    "\n",
    "# add datasets used for training and validation \n",
    "cfg.DATASETS.TRAIN = (\"birds_only_train\",)\n",
    "cfg.DATASETS.TEST = (\"birds_only_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "cfg.SOLVER.BASE_LR = 1e-3 # pick a good LR\n",
    "cfg.SOLVER.GAMMA = 0.1 # pick a good LR\n",
    "cfg.SOLVER.WARMUP_ITERS = 1\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = [500,]\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "\n",
    "cfg.OUTPUT_DIR = f\"./output/multibirds_{model_name}\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# train on bird species\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfDQZrLuKkn_"
   },
   "source": [
    "#### Bird species \n",
    "\n",
    "The bird species model both localizes and classifies bird species. We registered the species to be classifed in the above dataloader (see BIRD_SPECIES list). We utilize Faster R-CNN for better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmB4XbGUdI0u"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from Audubon_F21.utils.trainer import Trainer\n",
    "\n",
    "# setup training logger \n",
    "setup_logger()\n",
    "\n",
    "model_name = \"faster_rcnn_R_50_FPN_1x\"\n",
    "\n",
    "# Create detectron2 config \n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection/{model_name}.yaml\"))\n",
    "# Get pretrained model from MS COCO\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection/{model_name}.yaml\")\n",
    "\n",
    "# add datasets used for training and validation \n",
    "cfg.DATASETS.TRAIN = (\"birds_species_train\",)\n",
    "cfg.DATASETS.TEST = (\"birds_species_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "cfg.SOLVER.BASE_LR = 1e-3 # pick a good LR\n",
    "cfg.SOLVER.GAMMA = 0.1 # pick a good LR\n",
    "cfg.SOLVER.WARMUP_ITERS = 1\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES)\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = [500,]\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "\n",
    "cfg.OUTPUT_DIR = f\"./output/multibirds_{model_name}\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# train on bird species\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8liZIl4GCvV"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The following cell outputs various evaluation metrics, plots, and images. Please read more about the [COCO evaluation metrics](https://cocodataset.org/#detection-eval) to understand how the AP metrics are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE1OYaOPKf1s"
   },
   "source": [
    "#### Bird-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi41HGjtKf-_"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from Audubon_F21.utils.evaluation import plot_precision_recall\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./output/bird_only_retinanet_R_50_FPN_1x/model_final.pth\" # path to the model we just trained\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "print('validation inference:')\n",
    "val_precisions, val_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_only_val\")\n",
    "plot_precision_recall(val_precisions, val_max_recalls, [\"Bird\"])\n",
    "\n",
    "print('test inference:')\n",
    "test_precisions, test_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_only_test\")\n",
    "plot_precision_recall(test_precisions, test_max_recalls, [\"Bird\"])\n",
    "\n",
    "# Plot examples of detections on validation and testing tiled images \n",
    "for d in [\"val\", \"test\"]:\n",
    "    dataset_dicts = DatasetCatalog.get(f\"birds_only_{d}\")\n",
    "    print(f'\\n {d} examples:')\n",
    "    for k in random.sample(dataset_dicts, 2):\n",
    "        im = cv2.imread(k[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        outputs = outputs[\"instances\"].to(\"cpu\")\n",
    "        outputs = outputs[outputs.scores > 0.5]\n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                        metadata=MetadataCatalog.get(f\"birds_only_{d}\"),\n",
    "                        scale=0.5)\n",
    "        out = v.draw_instance_predictions(outputs)\n",
    "        cv2.imshow(f'{d} prediction {i}',out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmrAC_sTKd77"
   },
   "source": [
    "#### Bird species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkypxNYPqUW3"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from Audubon_F21.utils.evaluation import plot_precision_recall\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"./output/multibirds_faster_rcnn_R_50_FPN_1x/model_final.pth\" # path to the model we just trained\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "print('validation inference:')\n",
    "val_precisions, val_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_species_val\")\n",
    "plot_precision_recall(val_precisions, val_max_recalls, BIRD_SPECIES + [\"Unknown Bird\"],\n",
    "                      BIRD_SPECIES_COLORS + [(0, 0, 0)])\n",
    "\n",
    "print('test inference:')\n",
    "test_precisions, test_max_recalls = get_precisions_recalls(cfg, predictor, \"birds_species_test\")\n",
    "plot_precision_recall(test_precisions, test_max_recalls, BIRD_SPECIES + [\"Unknown Bird\"],\n",
    "                      BIRD_SPECIES_COLORS + [(0, 0, 0)])\n",
    "\n",
    "# Plot examples of detections on validation and testing tiled images \n",
    "for d in [\"val\", \"test\"]:\n",
    "    dataset_dicts = DatasetCatalog.get(f\"birds_species_{d}\")\n",
    "    print(f'\\n {d} examples:')\n",
    "    for k in random.sample(dataset_dicts, 2):\n",
    "        im = cv2.imread(k[\"file_name\"])\n",
    "        outputs = predictor(im)\n",
    "        outputs = outputs[\"instances\"].to(\"cpu\")\n",
    "        outputs = outputs[outputs.scores > 0.5]\n",
    "        v = Visualizer(im[:, :, ::-1],\n",
    "                        metadata=MetadataCatalog.get(f\"birds_species_{d}\"),\n",
    "                        scale=0.5,\n",
    "                        instance_mode=ColorMode.SEGMENTATION)\n",
    "        out = v.draw_instance_predictions(outputs)\n",
    "        cv2.imshow(f'{d} prediction {i}',out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjFXDR2GFvG6"
   },
   "source": [
    "## Running trained model on dataset\n",
    "\n",
    "The following cells run a pretrained model on a dataset containing only raw images. It generates an output csv file containing the predicted bounding boxes after non-maximal suppression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVOe51dbBIgs"
   },
   "source": [
    "### Tiling\n",
    "\n",
    "The tiling step in the detection pipeline is done using a sliding window. The sub-images are deliberately generated to have a significant proportion of overlapping with adjacent sub-images. The level of overlapping can be specified by setting a parameter. The reason why we want to have the overlapping is because we can ensure that there is at least one complete version of each bird in one of the sub-images. We then try to eliminate overlapping predicted bounding boxes for the same bird by using non-maximum suppression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMbdS6L3Elbr"
   },
   "outputs": [],
   "source": [
    "from Audubon_F21.utils.cropping import crop_dataset_img_only\n",
    "\n",
    "# create folder to contain tiled images\n",
    "!rm -rf './data/crop'\n",
    "!mkdir -p './data/crop'\n",
    "\n",
    "# perform tiling on images \n",
    "data_dir = './data/raw' # data directory folder \n",
    "output_dir = './data/crop'\n",
    "img_ext = '.JPG'\n",
    "CROP_WIDTH = 640 \n",
    "CROP_HEIGHT = 640\n",
    "SLIDING_SIZE = 400 \n",
    "crop_dataset_img_only(data_dir, img_ext, output_dir, crop_height=CROP_HEIGHT, crop_width=CROP_WIDTH, sliding_size=SLIDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoowvdjRBK1w"
   },
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K62BnBAi9_EN"
   },
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from Audubon_F21.utils.evaluation import evaluate_full_pipeline\n",
    "\n",
    "# create list of tiled images to be run predictor on \n",
    "eval_file_lst = []\n",
    "eval_file_lst = eval_file_lst + glob.glob('./data/crop/*.JPEG')\n",
    "\n",
    "# Create detectron2 config and predictor \n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# download model weights\n",
    "!gdown -q https://drive.google.com/uc?id=1-f_INg5D0yG7AJUkuSJUcIl6BSaf-smR \n",
    "# load model weights \n",
    "cfg.MODEL.WEIGHTS = \"./model_final.pth\"\n",
    "\n",
    "BIRD_SPECIES = [\"Brown Pelican\", \"Laughing Gull\", \"Mixed Tern\",\n",
    "                \"Great Blue Heron\",\"Great Egret/White Morph\"]\n",
    "SPECIES_MAP = {0: 'Brown Pelican', 1: 'Laughing Gull', 2: 'Mixed Tern', 3: 'Great Blue Heron',\n",
    "               4: 'Great Egret/White Morph', 5: 'Other/Unknown'}\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(BIRD_SPECIES) \n",
    "\n",
    "# Create default predictor to run inference \n",
    "predictor = DefaultPredictor(cfg)\n",
    "RAW_IMG_WIDTH = 8192\n",
    "RAW_IMG_HEIGHT = 5460\n",
    "\n",
    "# Run evaluation \n",
    "output_df = evaluate_full_pipeline(eval_file_lst, predictor, SPECIES_MAP, RAW_IMG_WIDTH, RAW_IMG_HEIGHT,\n",
    "                           CROP_WIDTH, CROP_HEIGHT, SLIDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDetAeyFBPtP"
   },
   "source": [
    "### Download annotations as CSV file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrBccu--38Ho"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "output_df.to_csv('output.csv')\n",
    "files.download('output.csv') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtFjtOerH5KNQRHS/Mz6hg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Audubon-Bird-Detection-Tutorial.ipynb",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
